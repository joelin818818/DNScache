#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import dns.resolver
import requests
import socket
import json
import time
import re
import threading
import csv
import configparser
import copy
import random
import statistics
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
from queue import Queue

class DNSRateLimiter:
    """DNSæŸ¥è¯¢é€Ÿç‡é™åˆ¶å™¨ï¼Œç¡®ä¿æ¯ç§’ä¸è¶…è¿‡æŒ‡å®šæ¬¡æ•°çš„æŸ¥è¯¢"""
    def __init__(self, queries_per_second=12):
        self.queries_per_second = queries_per_second
        self.query_times = []
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…ä»¥æ»¡è¶³é€Ÿç‡é™åˆ¶"""
        with self.lock:
            current_time = time.time()
            
            # ç§»é™¤ä¸€ç§’å‰çš„æŸ¥è¯¢è®°å½•
            self.query_times = [t for t in self.query_times if current_time - t < 1.0]
            
            # å¦‚æœå½“å‰æŸ¥è¯¢æ¬¡æ•°å·²è¾¾åˆ°é™åˆ¶ï¼Œåˆ™ç­‰å¾…
            if len(self.query_times) >= self.queries_per_second:
                sleep_time = 1.0 - (current_time - self.query_times[0])
                if sleep_time > 0:
                    time.sleep(sleep_time)
            
            # è®°å½•å½“å‰æŸ¥è¯¢æ—¶é—´
            self.query_times.append(time.time())

class Config:
    """é…ç½®ç®¡ç†å™¨"""
    def __init__(self, config_file="config.ini"):
        self.config_file = config_file
        self.config = configparser.ConfigParser()
        # è®¾ç½®é»˜è®¤é…ç½®
        self.default_config = {
            'General': {
                'TargetCount': '2000',
                'DataDirectory': 'data',
            },
            'DNS': {
                'QueriesPerSecond': '12',
                'MaxWorkers': '12',
                'Timeout': '1',
                'BatchSize': '100',
            },
            'Crawler': {
                'ParseJavaScript': 'false',
                'ParseCSS': 'false',
                'ParseImages': 'false',
                'ParseMetaTags': 'false',
                'UserAgent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Timeout': '10',
                'CollectThreads': '5',  # åŸŸåæ”¶é›†çº¿ç¨‹æ•°
            },
            'Export': {
                'DefaultFormat': 'json',
                'IncludeDNSInfo': 'true',
            }
        }
        
        # é…ç½®é¡¹çš„ä¸­æ–‡åç§°å’Œæè¿°
        self.config_names = {
            'General': 'å¸¸è§„è®¾ç½®',
            'DNS': 'DNSæŸ¥è¯¢è®¾ç½®',
            'Crawler': 'ç½‘é¡µçˆ¬å–è®¾ç½®',
            'Export': 'å¯¼å‡ºè®¾ç½®',
            'TargetCount': 'ç›®æ ‡åŸŸåæ•°é‡',
            'DataDirectory': 'æ•°æ®å­˜å‚¨ç›®å½•',
            'QueriesPerSecond': 'æ¯ç§’æŸ¥è¯¢æ¬¡æ•°',
            'MaxWorkers': 'æœ€å¤§çº¿ç¨‹æ•°',
            'Timeout': 'è¶…æ—¶æ—¶é—´(ç§’)',
            'BatchSize': 'æ‰¹å¤„ç†å¤§å°',
            'ParseJavaScript': 'è§£æJavaScriptæ–‡ä»¶',
            'ParseCSS': 'è§£æCSSæ–‡ä»¶',
            'ParseImages': 'è§£æå›¾ç‰‡é“¾æ¥',
            'ParseMetaTags': 'è§£æMetaæ ‡ç­¾',
            'UserAgent': 'æµè§ˆå™¨æ ‡è¯†',
            'CollectThreads': 'åŸŸåæ”¶é›†çº¿ç¨‹æ•°',
            'DefaultFormat': 'é»˜è®¤å¯¼å‡ºæ ¼å¼',
            'IncludeDNSInfo': 'åŒ…å«DNSæŸ¥è¯¢ç»“æœ'
        }
        
        # é…ç½®é¡¹çš„è¯¦ç»†è¯´æ˜
        self.config_descriptions = {
            'TargetCount': 'è¦æ”¶é›†çš„åŸŸåæ€»æ•°',
            'DataDirectory': 'ç”¨äºå­˜å‚¨åŸŸåæ–‡ä»¶çš„ç›®å½•',
            'QueriesPerSecond': 'DNSæŸ¥è¯¢é€Ÿç‡é™åˆ¶ï¼ˆæ¯ç§’æœ€å¤šæŸ¥è¯¢æ¬¡æ•°ï¼‰',
            'MaxWorkers': 'DNSæŸ¥è¯¢ä½¿ç”¨çš„æœ€å¤§çº¿ç¨‹æ•°',
            'Timeout': 'DNSæŸ¥è¯¢å’Œç½‘é¡µè¯·æ±‚çš„è¶…æ—¶æ—¶é—´',
            'BatchSize': 'æ¯æ‰¹å¤„ç†çš„åŸŸåæ•°é‡',
            'ParseJavaScript': 'æ˜¯å¦ä»JavaScriptæ–‡ä»¶ä¸­æå–åŸŸåï¼ˆtrue/falseï¼‰',
            'ParseCSS': 'æ˜¯å¦ä»CSSæ–‡ä»¶ä¸­æå–åŸŸåï¼ˆtrue/falseï¼‰',
            'ParseImages': 'æ˜¯å¦ä»å›¾ç‰‡é“¾æ¥ä¸­æå–åŸŸåï¼ˆtrue/falseï¼‰',
            'ParseMetaTags': 'æ˜¯å¦ä»Metaæ ‡ç­¾ä¸­æå–åŸŸåï¼ˆtrue/falseï¼‰',
            'UserAgent': 'è®¿é—®ç½‘é¡µæ—¶ä½¿ç”¨çš„æµè§ˆå™¨æ ‡è¯†',
            'CollectThreads': 'åŸŸåæ”¶é›†è¿‡ç¨‹ä½¿ç”¨çš„çº¿ç¨‹æ•°',
            'DefaultFormat': 'é»˜è®¤çš„ç»“æœå¯¼å‡ºæ ¼å¼ï¼ˆjson/csvï¼‰',
            'IncludeDNSInfo': 'ä¿å­˜åŸŸåæ–‡ä»¶æ—¶æ˜¯å¦åŒ…å«DNSæŸ¥è¯¢ç»“æœï¼ˆtrue/falseï¼‰'
        }
        
        self.load_config()
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        # å…ˆè®¾ç½®é»˜è®¤é…ç½®
        for section, options in self.default_config.items():
            if not self.config.has_section(section):
                self.config.add_section(section)
            for option, value in options.items():
                self.config.set(section, option, value)
        
        # å°è¯•ä»æ–‡ä»¶åŠ è½½é…ç½®
        if os.path.exists(self.config_file):
            try:
                self.config.read(self.config_file, encoding='utf-8')
                print(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {self.config_file}")
            except Exception as e:
                print(f"åŠ è½½é…ç½®æ–‡ä»¶å‡ºé”™: {e}")
        else:
            # ä¿å­˜é»˜è®¤é…ç½®
            self.save_config()
            print(f"å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {self.config_file}")
    
    def save_config(self):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                self.config.write(f)
            print(f"é…ç½®å·²ä¿å­˜åˆ°: {self.config_file}")
        except Exception as e:
            print(f"ä¿å­˜é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def get(self, section, option, fallback=None):
        """è·å–é…ç½®å€¼"""
        return self.config.get(section, option, fallback=fallback)
    
    def getint(self, section, option, fallback=None):
        """è·å–æ•´æ•°é…ç½®å€¼"""
        return self.config.getint(section, option, fallback=fallback)
    
    def getfloat(self, section, option, fallback=None):
        """è·å–æµ®ç‚¹æ•°é…ç½®å€¼"""
        return self.config.getfloat(section, option, fallback=fallback)
    
    def getboolean(self, section, option, fallback=None):
        """è·å–å¸ƒå°”é…ç½®å€¼"""
        return self.config.getboolean(section, option, fallback=fallback)
    
    def set(self, section, option, value):
        """è®¾ç½®é…ç½®å€¼"""
        if not self.config.has_section(section):
            self.config.add_section(section)
        self.config.set(section, option, str(value))
    
    def get_name(self, key):
        """è·å–é…ç½®é¡¹çš„ä¸­æ–‡åç§°"""
        return self.config_names.get(key, key)
    
    def get_description(self, key):
        """è·å–é…ç½®é¡¹çš„ä¸­æ–‡æè¿°"""
        return self.config_descriptions.get(key, "")

class DNSPerformanceTester:
    """DNSæ€§èƒ½æµ‹è¯•å·¥å…·ï¼Œç”¨äºæµ‹è¯•ä¸åŒå‚æ•°ä¸‹çš„æ€§èƒ½è¡¨ç°"""
    
    def __init__(self, test_domains_file=None, output_dir="test_results", config=None):
        # å›ºå®šæµ‹è¯•æ•°æ®
        self.test_domains = []
        self.load_test_domains(test_domains_file)
        
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        self.output_dir = output_dir
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
        
        # ä½¿ç”¨ä¼ å…¥çš„é…ç½®æˆ–åˆ›å»ºæ–°é…ç½®
        self.config = config
        
        # åˆå§‹é»˜è®¤å‚æ•°
        self.default_params = {
            'QueriesPerSecond': self.config.getint('DNS', 'QueriesPerSecond') if config else 12,
            'MaxWorkers': self.config.getint('DNS', 'MaxWorkers') if config else 12,
            'Timeout': self.config.getfloat('DNS', 'Timeout') if config else 1,
            'BatchSize': self.config.getint('DNS', 'BatchSize') if config else 100,
            'CollectThreads': self.config.getint('Crawler', 'CollectThreads') if config else 5
        }
        
        # å‚æ•°æµ‹è¯•èŒƒå›´
        self.param_ranges = {
            'QueriesPerSecond': [5, 10, 15, 20, 25, 30],
            'MaxWorkers': [5, 10, 15, 20, 30, 50],
            'Timeout': [0.5, 1, 2, 3, 5],
            'BatchSize': [50, 100, 200, 500],
            'CollectThreads': [3, 5, 10, 15, 20]
        }
        
        # å‚æ•°åç§°æ˜ å°„
        self.param_names = {
            'QueriesPerSecond': 'æ¯ç§’æŸ¥è¯¢æ¬¡æ•° (QueriesPerSecond)',
            'MaxWorkers': 'æœ€å¤§çº¿ç¨‹æ•° (MaxWorkers)',
            'Timeout': 'æŸ¥è¯¢è¶…æ—¶æ—¶é—´ (Timeout)',
            'BatchSize': 'æ‰¹å¤„ç†å¤§å° (BatchSize)',
            'CollectThreads': 'åŸŸåæ”¶é›†çº¿ç¨‹æ•° (CollectThreads)'
        }
        
        # å‚æ•°ç®€ç§°æ˜ å°„
        self.param_short_names = {
            'QueriesPerSecond': 'æ¯ç§’æŸ¥è¯¢æ¬¡æ•°',
            'MaxWorkers': 'æœ€å¤§çº¿ç¨‹æ•°',
            'Timeout': 'è¶…æ—¶æ—¶é—´(ç§’)',
            'BatchSize': 'æ‰¹å¤„ç†å¤§å°',
            'CollectThreads': 'æ”¶é›†çº¿ç¨‹æ•°'
        }
        
        # æ€§èƒ½æµ‹è¯•ç»“æœ
        self.results = []
        self.all_param_results = {}
        
        # é€Ÿç‡é™åˆ¶å™¨
        self.query_limiter = None
    
    def load_test_domains(self, file_path=None):
        """åŠ è½½æµ‹è¯•åŸŸå"""
        # å¦‚æœæä¾›äº†æ–‡ä»¶ï¼Œä»æ–‡ä»¶åŠ è½½åŸŸå
        if file_path and os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        self.test_domains = data
                    elif isinstance(data, dict) and 'domains' in data:
                        self.test_domains = data['domains']
                    print(f"ä»æ–‡ä»¶åŠ è½½äº† {len(self.test_domains)} ä¸ªæµ‹è¯•åŸŸå")
            except Exception as e:
                print(f"åŠ è½½æµ‹è¯•åŸŸåæ–‡ä»¶å‡ºé”™: {e}")
        
        # å¦‚æœæ²¡æœ‰åŠ è½½åˆ°åŸŸåæˆ–æ²¡æä¾›æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤åŸŸå
        if not self.test_domains:
            # ä½¿ç”¨ä¸€äº›å¸¸ç”¨åŸŸåä½œä¸ºæµ‹è¯•æ•°æ®
            self.test_domains = [
                "baidu.com", "qq.com", "163.com", "taobao.com", "jd.com",
                "weibo.com", "sina.com.cn", "sohu.com", "douyin.com", "zhihu.com",
                "bilibili.com", "360.cn", "csdn.net", "github.com", "aliyun.com",
                "tencent.com", "ctrip.com", "xinhuanet.com", "huawei.com", "mi.com"
            ]
            
            # ä¸ºäº†è·å¾—æ›´å¤šæµ‹è¯•æ•°æ®ï¼Œæ·»åŠ å­åŸŸåå˜ä½“
            domain_variants = []
            prefixes = ["www", "mail", "blog", "news", "shop", "m", "api", "dev"]
            for domain in self.test_domains[:]:
                for prefix in prefixes:
                    domain_variants.append(f"{prefix}.{domain}")
            
            # åˆå¹¶åŸå§‹åŸŸåå’Œå˜ä½“
            self.test_domains.extend(domain_variants)
            print(f"ä½¿ç”¨ {len(self.test_domains)} ä¸ªé»˜è®¤æµ‹è¯•åŸŸå")

    class QueryRateLimiter:
        """æŸ¥è¯¢é€Ÿç‡é™åˆ¶å™¨ï¼Œç¡®ä¿æ¯ç§’ä¸è¶…è¿‡æŒ‡å®šæ¬¡æ•°çš„æŸ¥è¯¢"""
        def __init__(self, queries_per_second):
            self.queries_per_second = queries_per_second
            self.query_times = []
            self.lock = threading.Lock()
        
        def wait_if_needed(self):
            """å¦‚æœéœ€è¦ï¼Œç­‰å¾…ä»¥æ»¡è¶³é€Ÿç‡é™åˆ¶"""
            with self.lock:
                current_time = time.time()
                
                # ç§»é™¤ä¸€ç§’å‰çš„æŸ¥è¯¢è®°å½•
                self.query_times = [t for t in self.query_times if current_time - t < 1.0]
                
                # å¦‚æœå½“å‰æŸ¥è¯¢æ¬¡æ•°å·²è¾¾åˆ°é™åˆ¶ï¼Œåˆ™ç­‰å¾…
                if len(self.query_times) >= self.queries_per_second:
                    sleep_time = 1.0 - (current_time - self.query_times[0])
                    if sleep_time > 0:
                        time.sleep(sleep_time)
                
                # è®°å½•å½“å‰æŸ¥è¯¢æ—¶é—´
                self.query_times.append(time.time())
    
    def query_dns(self, domain, timeout=1.0, rate_limiter=None):
        """æŸ¥è¯¢åŸŸåçš„DNSè®°å½•"""
        # åº”ç”¨é€Ÿç‡é™åˆ¶
        if rate_limiter:
            rate_limiter.wait_if_needed()
        
        start_time = time.time()
        result = {
            'domain': domain,
            'success': False,
            'query_time': 0,
            'error': None
        }
        
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨socket
            socket.gethostbyname(domain)
            result['success'] = True
        except:
            try:
                # å°è¯•ä½¿ç”¨dns.resolver
                resolver = dns.resolver.Resolver()
                resolver.timeout = timeout
                resolver.lifetime = timeout
                answers = resolver.resolve(domain, 'A')
                result['success'] = True
            except Exception as e:
                result['error'] = str(e)
        
        # è®¡ç®—æŸ¥è¯¢æ—¶é—´
        result['query_time'] = time.time() - start_time
        return result
    
    def test_parameter(self, param_name, param_value):
        """æµ‹è¯•å•ä¸ªå‚æ•°çš„æ€§èƒ½"""
        # åˆ›å»ºä¸€ä¸ªåŸºäºé»˜è®¤å‚æ•°çš„æµ‹è¯•é…ç½®
        test_params = copy.deepcopy(self.default_params)
        test_params[param_name] = param_value
        
        # éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†åŸŸåè¿›è¡Œæµ‹è¯•ï¼Œä»¥æ§åˆ¶æµ‹è¯•æ—¶é—´
        test_sample = random.sample(self.test_domains, min(100, len(self.test_domains)))
        
        print(f"æµ‹è¯•å‚æ•° {self.param_names[param_name]} = {param_value}, ä½¿ç”¨ {len(test_sample)} ä¸ªåŸŸå...")
        
        # è®¾ç½®é€Ÿç‡é™åˆ¶å™¨
        self.query_limiter = self.QueryRateLimiter(test_params['QueriesPerSecond'])
        
        # è®¾ç½®DNSè¶…æ—¶
        timeout = test_params['Timeout']
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # å¹¶è¡ŒæŸ¥è¯¢DNS
        results = []
        with ThreadPoolExecutor(max_workers=test_params['MaxWorkers']) as executor:
            # åˆ†æ‰¹å¤„ç†
            batch_size = test_params['BatchSize']
            for i in range(0, len(test_sample), batch_size):
                batch = test_sample[i:i+batch_size]
                
                # æäº¤æ‰¹é‡æŸ¥è¯¢ä»»åŠ¡
                futures = []
                for domain in batch:
                    future = executor.submit(self.query_dns, domain, timeout, self.query_limiter)
                    futures.append(future)
                
                # è·å–ç»“æœ
                for future in futures:
                    result = future.result()
                    results.append(result)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_time = time.time() - start_time
        successful_queries = sum(1 for r in results if r['success'])
        success_rate = successful_queries / len(test_sample) if test_sample else 0
        
        # è®¡ç®—æŸ¥è¯¢æ—¶é—´ç»Ÿè®¡æ•°æ®
        query_times = [r['query_time'] for r in results]
        avg_query_time = statistics.mean(query_times) if query_times else 0
        
        try:
            median_query_time = statistics.median(query_times) if query_times else 0
        except:
            median_query_time = 0
        
        # æ¯ç§’æŸ¥è¯¢æ•°
        queries_per_second = len(test_sample) / total_time if total_time > 0 else 0
        
        # è®°å½•æµ‹è¯•ç»“æœ
        test_result = {
            'param_name': param_name,
            'param_value': param_value,
            'total_time': total_time,
            'success_rate': success_rate,
            'avg_query_time': avg_query_time,
            'median_query_time': median_query_time,
            'queries_per_second': queries_per_second,
            'sample_size': len(test_sample)
        }
        
        print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’, æˆåŠŸç‡: {success_rate:.2%}, å¹³å‡æŸ¥è¯¢æ—¶é—´: {avg_query_time:.4f}ç§’, æ¯ç§’æŸ¥è¯¢æ•°: {queries_per_second:.2f}")
        self.results.append(test_result)
        
        return test_result
    
    def run_tests(self):
        """è¿è¡Œæ‰€æœ‰å‚æ•°æµ‹è¯•"""
        print("å¼€å§‹DNSç¼“å­˜å·¥å…·å‚æ•°æ€§èƒ½æµ‹è¯•...\n")
        
        # ä¸ºæ¯ä¸ªå‚æ•°æµ‹è¯•ä¸åŒçš„å€¼
        for param_name, values in self.param_ranges.items():
            print(f"\næµ‹è¯•å‚æ•°: {self.param_names[param_name]}")
            print("="*50)
            
            param_results = []
            for value in values:
                test_result = self.test_parameter(param_name, value)
                param_results.append(test_result)
            
            # æ‰¾å‡ºè¿™ä¸ªå‚æ•°çš„æœ€ä½³å€¼
            best_result = max(param_results, key=lambda x: x['queries_per_second'] * x['success_rate'])
            self.default_params[param_name] = best_result['param_value']
            
            print(f"\næœ€ä½³{self.param_names[param_name]}å€¼: {best_result['param_value']}")
            print(f"  æŸ¥è¯¢é€Ÿåº¦: {best_result['queries_per_second']:.2f}/ç§’, æˆåŠŸç‡: {best_result['success_rate']:.2%}")
            
            # ä¿å­˜è¿™ä¸ªå‚æ•°çš„å•ç‹¬æµ‹è¯•ç»“æœ
            self.save_param_results(param_name, param_results)
        
        # ä¿å­˜æœ€ç»ˆçš„æœ€ä½³å‚æ•°
        self.save_best_params()
    
    def save_param_results(self, param_name, results):
        """ä¿å­˜å•ä¸ªå‚æ•°çš„æµ‹è¯•ç»“æœ"""
        file_path = os.path.join(self.output_dir, f"param_test_{param_name}.json")
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2)
        print(f"å‚æ•° {self.param_names[param_name]} çš„æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {file_path}")
        
        # å­˜å‚¨æµ‹è¯•ç»“æœç”¨äºç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self.all_param_results[param_name] = results
    
    def save_readable_results(self):
        """å°†JSONæµ‹è¯•ç»“æœè½¬æ¢ä¸ºæ˜“è¯»çš„TXTæ ¼å¼"""
        output_file = os.path.join(self.output_dir, "dns_performance_test_results.txt")
        
        with open(output_file, "w", encoding="utf-8") as out_file:
            out_file.write("DNSç¼“å­˜å·¥å…·æ€§èƒ½æµ‹è¯•ç»“æœ\n")
            out_file.write("=" * 50 + "\n\n")
            
            # é¦–å…ˆæ˜¾ç¤ºæœ€ä½³å‚æ•°ç»„åˆ
            out_file.write("ã€æœ€ä½³å‚æ•°ç»„åˆã€‘\n")
            out_file.write("-" * 30 + "\n")
            for param, value in self.default_params.items():
                param_name = self.param_short_names.get(param, param)
                out_file.write(f"{param_name}: {value}\n")
            out_file.write("\n")
            
            # æ˜¾ç¤ºå„ä¸ªå‚æ•°çš„æµ‹è¯•ç»“æœ
            for param_name, results in self.all_param_results.items():
                short_name = self.param_short_names.get(param_name, param_name)
                out_file.write(f"ã€{short_name}å‚æ•°æµ‹è¯•ç»“æœã€‘\n")
                out_file.write("-" * 30 + "\n")
                out_file.write(f"{'å‚æ•°å€¼':<10}{'æ€»è€—æ—¶(ç§’)':<15}{'æˆåŠŸç‡':<10}{'æ¯ç§’æŸ¥è¯¢æ•°':<15}\n")
                
                # æŒ‰æ€§èƒ½æŒ‡æ ‡(æ¯ç§’æŸ¥è¯¢æ•°*æˆåŠŸç‡)æ’åº
                sorted_results = sorted(results, key=lambda x: x.get('queries_per_second', 0) * x.get('success_rate', 0), reverse=True)
                
                for result in sorted_results:
                    param_value = result.get('param_value', 'N/A')
                    total_time = f"{result.get('total_time', 0):.2f}"
                    success_rate = f"{result.get('success_rate', 0)*100:.1f}%"
                    qps = f"{result.get('queries_per_second', 0):.2f}"
                    
                    out_file.write(f"{param_value:<10}{total_time:<15}{success_rate:<10}{qps:<15}\n")
                
                out_file.write("\n")
            
            # æµ‹è¯•ç»“è®º
            out_file.write("æµ‹è¯•ç»“è®º\n")
            out_file.write("-" * 30 + "\n")
            out_file.write("ä»¥ä¸Šå‚æ•°ç»„åˆåœ¨å½“å‰ç³»ç»Ÿå’Œç½‘ç»œç¯å¢ƒä¸‹ç»è¿‡æµ‹è¯•åå¾—å‡ºçš„æœ€ä½³æ€§èƒ½å‚æ•°è®¾ç½®ã€‚\n")
            out_file.write("ä½¿ç”¨è¿™äº›å‚æ•°å¯ä»¥åœ¨ä¿æŒè¾ƒé«˜DNSæŸ¥è¯¢æˆåŠŸç‡çš„åŒæ—¶ï¼Œè·å¾—æœ€ä¼˜çš„æŸ¥è¯¢æ€§èƒ½ã€‚\n\n")
            out_file.write(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        print(f"\næµ‹è¯•ç»“æœå·²è½¬æ¢ä¸ºæ˜“è¯»æ ¼å¼ï¼Œä¿å­˜è‡³: {output_file}")
        return output_file
    
    def save_best_params(self):
        """ä¿å­˜æœ€ä½³å‚æ•°ç»„åˆ"""
        file_path = os.path.join(self.output_dir, "best_params.json")
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(self.default_params, f, indent=2)
        
        # åŒæ—¶åˆ›å»ºä¸€ä¸ªé…ç½®æ–‡ä»¶æ ¼å¼çš„ç‰ˆæœ¬
        config_content = """[General]
TargetCount = 2000
DataDirectory = data

[DNS]
QueriesPerSecond = {QueriesPerSecond}
MaxWorkers = {MaxWorkers}
Timeout = {Timeout}
BatchSize = {BatchSize}

[Crawler]
ParseJavaScript = false
ParseCSS = false
ParseImages = false
ParseMetaTags = false
UserAgent = Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36
Timeout = 10
CollectThreads = {CollectThreads}

[Export]
DefaultFormat = json
IncludeDNSInfo = true
""".format(**self.default_params)
        
        config_path = os.path.join(self.output_dir, "optimal_config.ini")
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        # ç”Ÿæˆæ˜“è¯»çš„TXTæ ¼å¼æµ‹è¯•ç»“æœ
        readable_result_file = self.save_readable_results()
        
        print(f"\næœ€ä½³å‚æ•°å·²ä¿å­˜åˆ°: {file_path}")
        print(f"ä¼˜åŒ–åçš„é…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {config_path}")
        print(f"æ˜“è¯»æ ¼å¼çš„æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {readable_result_file}")
        
        # æ‰“å°æœ€ç»ˆæ¨è
        self.print_recommendations()
    
    def print_recommendations(self):
        """æ‰“å°æœ€ç»ˆæ¨èçš„å‚æ•°è®¾ç½®"""
        print("\n" + "="*60)
        print("DNSç¼“å­˜å·¥å…·æ€§èƒ½æµ‹è¯•å®Œæˆï¼Œæ¨èå‚æ•°è®¾ç½®:")
        print("="*60)
        
        print(f"æ¯ç§’æŸ¥è¯¢æ¬¡æ•° (QueriesPerSecond): {self.default_params['QueriesPerSecond']}")
        print(f"æœ€å¤§çº¿ç¨‹æ•° (MaxWorkers): {self.default_params['MaxWorkers']}")
        print(f"DNSæŸ¥è¯¢è¶…æ—¶æ—¶é—´ (Timeout): {self.default_params['Timeout']}ç§’")
        print(f"æ‰¹å¤„ç†å¤§å° (BatchSize): {self.default_params['BatchSize']}ä¸ªåŸŸå")
        print(f"åŸŸåæ”¶é›†çº¿ç¨‹æ•° (CollectThreads): {self.default_params['CollectThreads']}")
        
        print("\nè¿™äº›å‚æ•°åœ¨å½“å‰ç³»ç»Ÿå’Œç½‘ç»œç¯å¢ƒä¸‹åº”è¯¥å…·æœ‰æœ€ä½³æ€§èƒ½ã€‚")
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦åº”ç”¨è®¾ç½®
        config_path = os.path.join(self.output_dir, "optimal_config.ini")
        target_config = "config.ini"
        
        while True:
            user_input = input(f"\næ˜¯å¦å°†ä¼˜åŒ–é…ç½® {config_path} å¤åˆ¶åˆ°ç¨‹åºç›®å½•å¹¶é‡å‘½åä¸º {target_config}ï¼Ÿ(y/n): ")
            if user_input.lower() in ['y', 'yes', 'æ˜¯', 'æ˜¯çš„']:
                try:
                    import shutil
                    shutil.copy2(config_path, target_config)
                    print(f"å·²æˆåŠŸå°†é…ç½®æ–‡ä»¶å¤åˆ¶ä¸º {target_config}")
                    
                    # æ›´æ–°å½“å‰é…ç½®
                    if self.config:
                        self.config.load_config()
                        print("é…ç½®å·²é‡æ–°åŠ è½½ï¼Œæ–°è®¾ç½®å°†åœ¨ä¸‹æ¬¡æ“ä½œæ—¶ç”Ÿæ•ˆ")
                except Exception as e:
                    print(f"å¤åˆ¶é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                break
            elif user_input.lower() in ['n', 'no', 'å¦', 'ä¸']:
                print(f"é…ç½®æ–‡ä»¶æœªåº”ç”¨ï¼Œæ‚¨å¯ä»¥ç¨åæ‰‹åŠ¨å¤åˆ¶ {config_path} åˆ°ç¨‹åºç›®å½•å¹¶é‡å‘½åä¸º {target_config}")
                break
            else:
                print("æ— æ•ˆçš„è¾“å…¥ï¼Œè¯·è¾“å…¥ y æˆ– n")

class DNSCacheTool:
    def __init__(self):
        # åŠ è½½é…ç½®
        self.config = Config()
        
        self.visited_domains = set()
        self.domains_to_visit = set()
        self.collected_domains = set()
        self.dns_results = {}  # å­˜å‚¨DNSè§£æç»“æœ
        self.only_subdomains = False  # æ˜¯å¦åªæ”¶é›†å­åŸŸå
        self.base_domain = None  # åŸºç¡€åŸŸå
        self.current_source_file = None  # å½“å‰ä½¿ç”¨çš„æºæ–‡ä»¶
        
        # ä»é…ç½®ä¸­è¯»å–è®¾ç½®
        self.target_count = self.config.getint('General', 'TargetCount')
        self.data_dir = self.config.get('General', 'DataDirectory')
        self.current_file = None
        self.rate_limiter = DNSRateLimiter(
            queries_per_second=self.config.getint('DNS', 'QueriesPerSecond')
        )
        
        # åˆ›å»ºæ•°æ®ç›®å½•
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
    
    def extract_domain(self, url):
        """ä»URLä¸­æå–åŸŸå"""
        try:
            if not url.startswith('http'):
                url = 'http://' + url
            parsed = urlparse(url)
            domain = parsed.netloc
            # ç§»é™¤ç«¯å£å·
            if ':' in domain:
                domain = domain.split(':')[0]
            return domain.lower() if domain else None
        except Exception as e:
            print(f"è§£æURLæ—¶å‡ºé”™: {url}, é”™è¯¯: {e}")
            return None
    
    def is_subdomain(self, domain):
        """åˆ¤æ–­åŸŸåæ˜¯å¦ä¸ºæŒ‡å®šåŸºç¡€åŸŸåçš„å­åŸŸå"""
        if not self.only_subdomains or not self.base_domain:
            return True  # å¦‚æœä¸é™åˆ¶å­åŸŸåï¼Œåˆ™è¿”å›True
        
        # æ£€æŸ¥domainæ˜¯å¦ä»¥base_domainç»“å°¾
        return domain.endswith('.' + self.base_domain) or domain == self.base_domain
    
    def get_links_from_domain(self, domain):
        """è·å–åŸŸåé¡µé¢ä¸Šçš„æ‰€æœ‰é“¾æ¥å¹¶å¢å¼ºåŸŸåæå–èƒ½åŠ›"""
        links = set()
        try:
            headers = {
                'User-Agent': self.config.get('Crawler', 'UserAgent')
            }
            url = f"http://{domain}"
            
            # è®¿é—®ç½‘é¡µæ—¶å·²è¿›è¡ŒDNSè§£æ
            response = requests.get(
                url, 
                headers=headers, 
                timeout=self.config.getint('Crawler', 'Timeout')
            )
            
            # è®°å½•è‡ªåŠ¨å®Œæˆçš„DNSè§£æç»“æœ
            try:
                ip_address = socket.gethostbyname(domain)
                result = {
                    'domain': domain,
                    'success': True,
                    'ip_addresses': [ip_address],
                    'timestamp': time.time(),
                    'error': None
                }
                self.dns_results[domain] = result
            except Exception as dns_error:
                # DNSè§£æå¤±è´¥ä½†ç½‘é¡µè®¿é—®æˆåŠŸçš„æƒ…å†µï¼ˆæå°‘å‘ç”Ÿï¼‰
                pass
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. ä»<a>æ ‡ç­¾æå–é“¾æ¥
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                extracted_domain = self.extract_domain(href)
                if extracted_domain and extracted_domain != domain:
                    if self.is_subdomain(extracted_domain):
                        links.add(extracted_domain)
            
            # 2. å¦‚æœé…ç½®å…è®¸ï¼Œä»<script>æ ‡ç­¾çš„srcå±æ€§ä¸­æå–åŸŸå
            if self.config.getboolean('Crawler', 'ParseJavaScript'):
                for script_tag in soup.find_all('script', src=True):
                    src = script_tag['src']
                    if src:
                        # å¤„ç†ç›¸å¯¹URL
                        full_url = urljoin(url, src)
                        extracted_domain = self.extract_domain(full_url)
                        if extracted_domain and extracted_domain != domain:
                            if self.is_subdomain(extracted_domain):
                                links.add(extracted_domain)
            
            # 3. å¦‚æœé…ç½®å…è®¸ï¼Œä»<link>æ ‡ç­¾ï¼ˆCSSæ–‡ä»¶ï¼‰ä¸­æå–åŸŸå
            if self.config.getboolean('Crawler', 'ParseCSS'):
                for link_tag in soup.find_all('link', href=True):
                    href = link_tag['href']
                    if href:
                        # å¤„ç†ç›¸å¯¹URL
                        full_url = urljoin(url, href)
                        extracted_domain = self.extract_domain(full_url)
                        if extracted_domain and extracted_domain != domain:
                            if self.is_subdomain(extracted_domain):
                                links.add(extracted_domain)
            
            # 4. å¦‚æœé…ç½®å…è®¸ï¼Œä»<img>æ ‡ç­¾çš„srcå±æ€§ä¸­æå–åŸŸå
            if self.config.getboolean('Crawler', 'ParseImages'):
                for img_tag in soup.find_all('img', src=True):
                    src = img_tag['src']
                    if src:
                        # å¤„ç†ç›¸å¯¹URL
                        full_url = urljoin(url, src)
                        extracted_domain = self.extract_domain(full_url)
                        if extracted_domain and extracted_domain != domain:
                            if self.is_subdomain(extracted_domain):
                                links.add(extracted_domain)
            
            # 5. å¦‚æœé…ç½®å…è®¸ï¼Œä»<meta>æ ‡ç­¾ä¸­æå–åŸŸå
            if self.config.getboolean('Crawler', 'ParseMetaTags'):
                for meta_tag in soup.find_all('meta', content=True):
                    content = meta_tag['content']
                    if content and ('http://' in content or 'https://' in content):
                        extracted_domain = self.extract_domain(content)
                        if extracted_domain and extracted_domain != domain:
                            if self.is_subdomain(extracted_domain):
                                links.add(extracted_domain)
            
            # 6. ä»JavaScriptæ–‡ä»¶ä¸­æå–URLæ¨¡å¼
            if self.config.getboolean('Crawler', 'ParseJavaScript'):
                for script in soup.find_all('script'):
                    if script.string:
                        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…URL
                        js_content = script.string
                        # åŒ¹é…åŒå¼•å·æˆ–å•å¼•å·ä¸­çš„URL
                        url_patterns = re.findall(r'["\']https?://([^/\'"]+)[\'"]', js_content)
                        for pattern in url_patterns:
                            if pattern and pattern != domain:
                                if self.is_subdomain(pattern):
                                    links.add(pattern)
            
        except Exception as e:
            print(f"è·å–åŸŸå {domain} é“¾æ¥æ—¶å‡ºé”™: {e}")
            # è®°å½•å¤±è´¥çš„DNSè§£æ
            result = {
                'domain': domain,
                'success': False,
                'ip_addresses': [],
                'timestamp': time.time(),
                'error': str(e)
            }
            self.dns_results[domain] = result
        
        return links
    
    def query_dns(self, domain):
        """æŸ¥è¯¢åŸŸåçš„DNSè®°å½•å¹¶å­˜å‚¨ç»“æœ"""
        # åº”ç”¨é€Ÿç‡é™åˆ¶
        self.rate_limiter.wait_if_needed()
        
        result = {
            'domain': domain,
            'success': False,
            'ip_addresses': [],
            'timestamp': time.time(),
            'error': None
        }
        
        try:
            ip = socket.gethostbyname(domain)
            result['success'] = True
            result['ip_addresses'].append(ip)
        except Exception as socket_error:
            try:
                resolver = dns.resolver.Resolver()
                resolver.timeout = self.config.getfloat('DNS', 'Timeout')
                resolver.lifetime = self.config.getfloat('DNS', 'Timeout')
                answers = resolver.resolve(domain, 'A')
                
                result['success'] = True
                for rdata in answers:
                    result['ip_addresses'].append(str(rdata))
            except Exception as dns_error:
                result['success'] = False
                result['error'] = str(dns_error)
                print(f"æŸ¥è¯¢DNSæ—¶å‡ºé”™: {domain}, é”™è¯¯: {dns_error}")
        
        # å­˜å‚¨ç»“æœ
        self.dns_results[domain] = result
        return result['success']
    
    def process_domain(self, domain):
        """å¤„ç†å•ä¸ªåŸŸåçš„æ“ä½œï¼Œç”¨äºå¤šçº¿ç¨‹"""
        if domain in self.visited_domains:
            return
            
        self.visited_domains.add(domain)
        print(f"æ­£åœ¨å¤„ç†åŸŸå: {domain} [å·²æ”¶é›†: {len(self.collected_domains)}/{self.target_count}]")
        
        try:
            # è·å–è¯¥åŸŸåä¸Šçš„é“¾æ¥ï¼Œè®¿é—®ç½‘é¡µæ—¶ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡ŒDNSè§£æ
            new_links = self.get_links_from_domain(domain)
            
            with self.lock:
                # è®°å½•æˆåŠŸè®¿é—®çš„åŸŸå
                self.collected_domains.add(domain)
                
                # æ·»åŠ æ–°å‘ç°çš„åŸŸååˆ°å¾…è®¿é—®åˆ—è¡¨
                new_domains = {d for d in new_links if d not in self.visited_domains}
                self.domains_to_visit.update(new_domains)
                
                # æ¯æ”¶é›†100ä¸ªåŸŸåä¿å­˜ä¸€æ¬¡ï¼ˆä½¿ç”¨ä¸´æ—¶ä¿å­˜ï¼Œä¸æ›´æ–°æ–‡ä»¶åä¸­çš„è®¡æ•°ï¼‰
                if len(self.collected_domains) % 100 == 0:
                    print(f"å·²æ”¶é›† {len(self.collected_domains)} ä¸ªåŸŸå")
                    self.save_domains_to_file(final_save=False)
        except Exception as e:
            print(f"å¤„ç†åŸŸå {domain} æ—¶å‡ºé”™: {e}")
    
    def collect_domains(self, start_domain, only_subdomains=False):
        """ä»èµ·å§‹åŸŸåå¼€å§‹æ”¶é›†åŸŸå"""
        self.only_subdomains = only_subdomains
        self.base_domain = start_domain
        print(f"å¼€å§‹ä» {start_domain} æ”¶é›†åŸŸå...")
        
        if only_subdomains:
            print(f"ğŸ”’ ä»…æ”¶é›† {start_domain} çš„å­åŸŸå")
        
        self.visited_domains = set()
        self.domains_to_visit = {start_domain}
        self.collected_domains = set()
        self.dns_results = {}
        self.lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”
        
        # é‡ç½®å½“å‰æ–‡ä»¶åï¼Œè®©save_domains_to_fileåˆ›å»ºæ–°æ–‡ä»¶å
        self.current_file = None
        
        # åˆ›å»ºçº¿ç¨‹æ± 
        collect_threads = self.config.getint('Crawler', 'CollectThreads')
        print(f"ğŸ§µ ä½¿ç”¨ {collect_threads} ä¸ªçº¿ç¨‹è¿›è¡ŒåŸŸåæ”¶é›†")
        
        with ThreadPoolExecutor(max_workers=collect_threads) as executor:
            while self.domains_to_visit and len(self.collected_domains) < self.target_count:
                # å–å‡ºä¸€æ‰¹åŸŸåè¿›è¡Œå¤„ç†
                batch_size = min(collect_threads * 2, len(self.domains_to_visit))
                domains_batch = []
                
                for _ in range(batch_size):
                    if not self.domains_to_visit:
                        break
                    domains_batch.append(self.domains_to_visit.pop())
                
                # æäº¤åˆ°çº¿ç¨‹æ± 
                futures = [executor.submit(self.process_domain, domain) for domain in domains_batch]
                
                # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
                for future in futures:
                    future.result()
        
        # æœ€ç»ˆä¿å­˜ï¼Œæ›´æ–°åŸŸåæ•°é‡
        self.save_domains_to_file(final_save=True)
        print(f"åŸŸåæ”¶é›†å®Œæˆ! å…±æ”¶é›†äº† {len(self.collected_domains)} ä¸ªåŸŸå")
    
    def batch_query_dns(self, file_path=None):
        """æ‰¹é‡æŸ¥è¯¢DNSä»¥åŠ å¿«ç¼“å­˜"""
        domains = self.load_domains_from_file(file_path) if file_path else self.collected_domains
        
        if not domains:
            print("æ²¡æœ‰åŸŸåå¯ä¾›æŸ¥è¯¢!")
            return
        
        # è®°å½•æ¥æºæ–‡ä»¶ï¼Œç”¨äºç”Ÿæˆæ›´æœ‰æè¿°æ€§çš„å¯¼å‡ºæ–‡ä»¶å
        if file_path:
            self.current_source_file = file_path
        else:
            self.current_source_file = None
        
        # æ¸…ç©ºä¹‹å‰çš„DNSç»“æœ
        self.dns_results = {}
        
        print(f"å¼€å§‹æŸ¥è¯¢ {len(domains)} ä¸ªåŸŸåçš„DNS...")
        print(f"æ³¨æ„: æŸ¥è¯¢é€Ÿç‡é™åˆ¶ä¸ºæ¯ç§’æœ€å¤š{self.config.getint('DNS', 'QueriesPerSecond')}æ¬¡æŸ¥è¯¢")
        
        success_count = 0
        total_count = len(domains)
        
        # ä½¿ç”¨é™åˆ¶çº¿ç¨‹æ•°çš„çº¿ç¨‹æ± æ¥æ§åˆ¶å¹¶å‘æŸ¥è¯¢
        max_workers = min(self.config.getint('DNS', 'MaxWorkers'), len(domains))
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            domains_list = list(domains)
            batch_size = self.config.getint('DNS', 'BatchSize')  # æ¯æ‰¹å¤„ç†çš„åŸŸåæ•°é‡
            
            for i in range(0, len(domains_list), batch_size):
                batch = domains_list[i:i+batch_size]
                results = list(executor.map(self.query_dns, batch))
                batch_success = sum(1 for r in results if r)
                success_count += batch_success
                
                # æ‰“å°è¿›åº¦
                progress = min(100, int((i + len(batch)) / total_count * 100))
                print(f"è¿›åº¦: {progress}%, æˆåŠŸ: {success_count}/{i+len(batch)}")
        
        print(f"DNSæŸ¥è¯¢å®Œæˆ! æˆåŠŸæŸ¥è¯¢äº† {success_count}/{total_count} ä¸ªåŸŸå")
        
        # è¯¢é—®æ˜¯å¦å¯¼å‡ºç»“æœ
        self.ask_export_results()
    
    def ask_export_results(self):
        """è¯¢é—®ç”¨æˆ·æ˜¯å¦å¯¼å‡ºç»“æœ"""
        if not self.dns_results:
            print("æ²¡æœ‰å¯å¯¼å‡ºçš„ç»“æœ!")
            return
        
        while True:
            print("\næ˜¯å¦å¯¼å‡ºDNSæŸ¥è¯¢ç»“æœ?")
            print("1. ğŸ“Š å¯¼å‡ºä¸ºJSONæ ¼å¼")
            print("2. ğŸ“ˆ å¯¼å‡ºä¸ºCSVæ ¼å¼")
            print("3. âŒ ä¸å¯¼å‡º")
            
            choice = input("\nè¯·é€‰æ‹© (1-3): ")
            
            if choice == '1':
                self.export_results('json')
                break
            elif choice == '2':
                self.export_results('csv')
                break
            elif choice == '3':
                break
            else:
                print("æ— æ•ˆçš„é€‰æ‹©! è¯·é‡è¯•ã€‚")
    
    def export_results(self, format_type):
        """å¯¼å‡ºDNSæŸ¥è¯¢ç»“æœ"""
        if not self.dns_results:
            print("æ²¡æœ‰ç»“æœå¯å¯¼å‡º!")
            return
        
        timestamp = time.strftime("%Y%m%d%H%M")
        
        # åˆ›å»ºæ›´æœ‰æè¿°æ€§çš„æ–‡ä»¶å
        filename_parts = []
        successful_domains = [domain for domain, result in self.dns_results.items() if result['success']]
        
        # æ·»åŠ æºæ–‡ä»¶æˆ–åŸŸåä¿¡æ¯
        if hasattr(self, 'base_domain') and self.base_domain:
            # å¦‚æœæ˜¯ä»ç‰¹å®šåŸŸåæ”¶é›†çš„
            base_name = self.base_domain.replace('.', '_')
            filename_parts.append(base_name)
            
            if hasattr(self, 'only_subdomains') and self.only_subdomains:
                filename_parts.append("ä»…å­åŸŸå")
        elif hasattr(self, 'current_source_file') and self.current_source_file:
            # å¦‚æœæ˜¯ä»æ–‡ä»¶åŠ è½½çš„
            source_filename = os.path.basename(self.current_source_file)
            source_name = os.path.splitext(source_filename)[0]
            filename_parts.append(f"æ¥æº_{source_name}")
        
        # æ·»åŠ æˆåŠŸæŸ¥è¯¢æ•°é‡ä¿¡æ¯
        filename_parts.append(f"{len(successful_domains)}ä¸ªæˆåŠŸDNSç»“æœ")
        
        # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†
        descriptive_name = "-".join(filename_parts) if filename_parts else "dns_results"
        
        if format_type.lower() == 'json':
            # å¯¼å‡ºä¸ºJSONï¼ŒåªåŒ…å«åŸŸååˆ—è¡¨
            export_file = os.path.join(self.data_dir, f"{descriptive_name}_{timestamp}.json")
            with open(export_file, 'w', encoding='utf-8') as f:
                json.dump(successful_domains, f, ensure_ascii=False, indent=2)
                
        elif format_type.lower() == 'csv':
            # å¯¼å‡ºä¸ºCSV
            export_file = os.path.join(self.data_dir, f"{descriptive_name}_{timestamp}.csv")
            with open(export_file, 'w', encoding='utf-8', newline='') as f:
                csv_writer = csv.writer(f)
                # å†™å…¥è¡¨å¤´
                csv_writer.writerow(['åŸŸå', 'è§£æçŠ¶æ€', 'IPåœ°å€'])
                
                # å†™å…¥æ•°æ®
                for domain, result in self.dns_results.items():
                    ip_addresses = ';'.join(result['ip_addresses']) if result['ip_addresses'] else ''
                    csv_writer.writerow([
                        domain,
                        'æˆåŠŸ' if result['success'] else 'å¤±è´¥',
                        ip_addresses
                    ])
        else:
            print(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format_type}")
            return
        
        print(f"ç»“æœå·²å¯¼å‡ºåˆ°: {export_file}")
        return export_file
    
    def save_domains_to_file(self, final_save=False):
        """ä¿å­˜åŸŸååˆ—è¡¨åˆ°æ–‡ä»¶
        
        å‚æ•°:
            final_save (bool): æ˜¯å¦æ˜¯æœ€ç»ˆä¿å­˜ï¼Œä¸ºTrueæ—¶ä¼šæ›´æ–°æ–‡ä»¶åä¸­çš„åŸŸåæ•°é‡
        """
        if not self.collected_domains:
            print("æ²¡æœ‰åŸŸåå¯ä¿å­˜!")
            return
        
        # å¦‚æœæ˜¯æœ€ç»ˆä¿å­˜æˆ–è€…å½“å‰æ–‡ä»¶æœªåˆ›å»ºï¼Œåˆ™åˆ›å»º/æ›´æ–°æ–‡ä»¶å
        if final_save or not self.current_file:
            timestamp = time.strftime("%Y%m%d%H%M")
            
            # åˆ›å»ºæ›´æœ‰æè¿°æ€§çš„æ–‡ä»¶å
            filename_parts = []
            
            # æ·»åŠ èµ·å§‹åŸŸåä¿¡æ¯
            if hasattr(self, 'base_domain') and self.base_domain:
                base_name = self.base_domain.replace('.', '_')
                filename_parts.append(base_name)
            
            # æ·»åŠ æ˜¯å¦åªåŒ…å«å­åŸŸåä¿¡æ¯
            if hasattr(self, 'only_subdomains') and self.only_subdomains:
                filename_parts.append("ä»…å­åŸŸå")
            
            # æ·»åŠ åŸŸåæ•°é‡
            filename_parts.append(f"{len(self.collected_domains)}ä¸ªåŸŸå")
            
            # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†
            descriptive_name = "-".join(filename_parts) if filename_parts else "domains"
            
            # å®Œæ•´æ–‡ä»¶å
            new_file = os.path.join(self.data_dir, f"{descriptive_name}_{timestamp}.json")
            
            # å¦‚æœå·²æœ‰æ–‡ä»¶ä¸”æ–‡ä»¶åä¸åŒï¼Œåˆ™éœ€è¦æ›´æ–°ï¼ˆé‡å‘½åæˆ–åˆ›å»ºæ–°æ–‡ä»¶ï¼‰
            if self.current_file and self.current_file != new_file and os.path.exists(self.current_file):
                # å¦‚æœæ˜¯æœ€ç»ˆä¿å­˜ï¼Œå°è¯•åˆ é™¤æ—§æ–‡ä»¶
                if final_save:
                    try:
                        os.remove(self.current_file)
                        print(f"å·²åˆ é™¤æ—§æ–‡ä»¶: {self.current_file}")
                    except Exception as e:
                        print(f"åˆ é™¤æ—§æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            
            self.current_file = new_file
        
        # ç®€åŒ–çš„æ ¼å¼ï¼Œä»…ä¿å­˜åŸŸååˆ—è¡¨
        domains_data = list(self.collected_domains)
        
        with open(self.current_file, 'w', encoding='utf-8') as f:
            json.dump(domains_data, f, ensure_ascii=False, indent=2)
        
        print(f"åŸŸåå·²ä¿å­˜åˆ°æ–‡ä»¶: {self.current_file}")
        return self.current_file
    
    def load_domains_from_file(self, file_path):
        """ä»æ–‡ä»¶åŠ è½½åŸŸååˆ—è¡¨"""
        try:
            # è®°å½•æ¥æºæ–‡ä»¶
            self.current_source_file = file_path
            
            with open(file_path, 'r', encoding='utf-8') as f:
                try:
                    # åŠ è½½JSONæ ¼å¼
                    data = json.load(f)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç›´æ¥çš„åŸŸååˆ—è¡¨ï¼ˆæ–°æ ¼å¼ï¼‰
                    if isinstance(data, list):
                        domains = data
                        print(f"ä»æ–‡ä»¶ {file_path} åŠ è½½äº† {len(domains)} ä¸ªåŸŸå")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ—§çš„å¤æ‚æ ¼å¼
                    elif isinstance(data, dict) and 'domains' in data:
                        domains = data['domains']
                        print(f"ä»æ–‡ä»¶ {file_path} åŠ è½½äº† {len(domains)} ä¸ªåŸŸå (æ—§æ ¼å¼)")
                        
                        # åŠ è½½å­åŸŸåè®¾ç½®
                        if 'only_subdomains' in data and 'base_domain' in data:
                            self.only_subdomains = data['only_subdomains']
                            self.base_domain = data['base_domain']
                            if self.only_subdomains and self.base_domain:
                                print(f"è¯¥æ–‡ä»¶ä»…åŒ…å« {self.base_domain} çš„å­åŸŸå")
                        
                        # å¦‚æœæ–‡ä»¶åŒ…å«DNSç»“æœä¿¡æ¯ï¼Œä¹ŸåŠ è½½
                        if 'dns_results' in data:
                            self.dns_results = data['dns_results']
                            print(f"åŒæ—¶åŠ è½½äº† {len(self.dns_results)} æ¡DNSæŸ¥è¯¢ç»“æœ")
                    
                    # å…¶ä»–æ ¼å¼
                    else:
                        domains = data
                        print(f"ä»æ–‡ä»¶ {file_path} åŠ è½½äº† {len(domains)} ä¸ªåŸŸå")
                        
                except:
                    # å°è¯•è¯»å–CSVæ ¼å¼
                    f.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
                    if file_path.endswith('.csv'):
                        csv_reader = csv.reader(f)
                        next(csv_reader)  # è·³è¿‡è¡¨å¤´
                        domains = []
                        for row in csv_reader:
                            if row and len(row) > 0:
                                domains.append(row[0])  # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯åŸŸå
                        print(f"ä»CSVæ–‡ä»¶ {file_path} åŠ è½½äº† {len(domains)} ä¸ªåŸŸå")
                    else:
                        raise ValueError("æ— æ³•è¯†åˆ«çš„æ–‡ä»¶æ ¼å¼")
                
                self.collected_domains = set(domains)
                return set(domains)
        except Exception as e:
            print(f"åŠ è½½åŸŸåæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return set()
    
    def import_domains(self):
        """å¯¼å…¥åŸŸååˆ—è¡¨"""
        print("\nğŸ“¥ å¯¼å…¥åŸŸååˆ—è¡¨")
        print("æ”¯æŒçš„æ ¼å¼: JSONæ–‡ä»¶æˆ–CSVæ–‡ä»¶ï¼ˆç¬¬ä¸€åˆ—ä¸ºåŸŸåï¼‰")
        
        file_path = input("è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„: ")
        
        if not os.path.exists(file_path):
            print("âŒ æ–‡ä»¶ä¸å­˜åœ¨!")
            return
        
        domains = self.load_domains_from_file(file_path)
        if domains:
            print(f"âœ… æˆåŠŸå¯¼å…¥ {len(domains)} ä¸ªåŸŸå")
            
            # è¯¢é—®æ˜¯å¦è¿›è¡ŒDNSæŸ¥è¯¢
            if input("\næ˜¯å¦å¯¹è¿™äº›åŸŸåè¿›è¡ŒDNSæŸ¥è¯¢? (y/n): ").lower() == 'y':
                self.batch_query_dns()
    
    def get_available_files(self):
        """è·å–å¯ç”¨çš„åŸŸåæ–‡ä»¶åˆ—è¡¨"""
        files = []
        for file in os.listdir(self.data_dir):
            if (file.startswith("domains_") and file.endswith(".json")) or \
               (file.startswith("dns_results_") and (file.endswith(".json") or file.endswith(".csv"))):
                files.append(os.path.join(self.data_dir, file))
        return files
    
    def edit_config(self):
        """ç¼–è¾‘é…ç½®"""
        while True:
            print("\n" + "="*50)
            print("âš™ï¸ é…ç½®è®¾ç½®")
            print("="*50)
            
            sections = self.config.config.sections()
            for i, section in enumerate(sections, 1):
                if section == 'General':
                    icon = "ğŸ”§"
                    name = "å¸¸è§„è®¾ç½®"
                elif section == 'DNS':
                    icon = "ğŸŒ"
                    name = "DNSæŸ¥è¯¢è®¾ç½®"
                elif section == 'Crawler':
                    icon = "ğŸ•¸ï¸"
                    name = "ç½‘é¡µçˆ¬å–è®¾ç½®"
                elif section == 'Export':
                    icon = "ğŸ“¤"
                    name = "å¯¼å‡ºè®¾ç½®"
                else:
                    icon = "ğŸ“"
                    name = section
                print(f"{i}. {icon} {name}")
            print(f"{len(sections)+1}. ğŸ’¾ ä¿å­˜å¹¶è¿”å›")
            
            try:
                choice = int(input("\nè¯·é€‰æ‹©è¦ç¼–è¾‘çš„éƒ¨åˆ†: "))
                
                if 1 <= choice <= len(sections):
                    section = sections[choice-1]
                    self.edit_section(section)
                elif choice == len(sections)+1:
                    self.config.save_config()
                    break
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©!")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")
    
    def edit_section(self, section):
        """ç¼–è¾‘ç‰¹å®šé…ç½®éƒ¨åˆ†"""
        # ç›´æ¥ç¿»è¯‘é…ç½®é¡¹
        key_translations = {
            'targetcount': 'ç›®æ ‡åŸŸåæ•°é‡',
            'datadirectory': 'æ•°æ®å­˜å‚¨ç›®å½•',
            'queriespersecond': 'æ¯ç§’æŸ¥è¯¢æ¬¡æ•°',
            'maxworkers': 'æœ€å¤§çº¿ç¨‹æ•°',
            'timeout': 'è¶…æ—¶æ—¶é—´(ç§’)',
            'batchsize': 'æ‰¹å¤„ç†å¤§å°',
            'parsejavascript': 'è§£æJavaScriptæ–‡ä»¶',
            'parsecss': 'è§£æCSSæ–‡ä»¶',
            'parseimages': 'è§£æå›¾ç‰‡é“¾æ¥',
            'parsemetatags': 'è§£æMetaæ ‡ç­¾',
            'useragent': 'æµè§ˆå™¨æ ‡è¯†',
            'collectthreads': 'åŸŸåæ”¶é›†çº¿ç¨‹æ•°',
            'defaultformat': 'é»˜è®¤å¯¼å‡ºæ ¼å¼',
            'includednsinfo': 'åŒ…å«DNSæŸ¥è¯¢ç»“æœ'
        }
        
        # ç›´æ¥ç¿»è¯‘æè¿°
        desc_translations = {
            'targetcount': 'è¦æ”¶é›†çš„åŸŸåæ€»æ•°',
            'datadirectory': 'ç”¨äºå­˜å‚¨åŸŸåæ–‡ä»¶çš„ç›®å½•',
            'queriespersecond': 'DNSæŸ¥è¯¢é€Ÿç‡é™åˆ¶ï¼ˆæ¯ç§’æœ€å¤šæŸ¥è¯¢æ¬¡æ•°ï¼‰',
            'maxworkers': 'DNSæŸ¥è¯¢ä½¿ç”¨çš„æœ€å¤§çº¿ç¨‹æ•°',
            'timeout': 'DNSæŸ¥è¯¢å’Œç½‘é¡µè¯·æ±‚çš„è¶…æ—¶æ—¶é—´',
            'batchsize': 'æ¯æ‰¹å¤„ç†çš„åŸŸåæ•°é‡',
            'parsejavascript': 'æ˜¯å¦ä»JavaScriptæ–‡ä»¶ä¸­æå–åŸŸåï¼ˆtrue/falseï¼‰',
            'parsecss': 'æ˜¯å¦ä»CSSæ–‡ä»¶ä¸­æå–åŸŸåï¼ˆtrue/falseï¼‰',
            'parseimages': 'æ˜¯å¦ä»å›¾ç‰‡é“¾æ¥ä¸­æå–åŸŸåï¼ˆtrue/falseï¼‰',
            'parsemetatags': 'æ˜¯å¦ä»Metaæ ‡ç­¾ä¸­æå–åŸŸåï¼ˆtrue/falseï¼‰',
            'useragent': 'è®¿é—®ç½‘é¡µæ—¶ä½¿ç”¨çš„æµè§ˆå™¨æ ‡è¯†',
            'collectthreads': 'åŸŸåæ”¶é›†è¿‡ç¨‹ä½¿ç”¨çš„çº¿ç¨‹æ•°',
            'defaultformat': 'é»˜è®¤çš„ç»“æœå¯¼å‡ºæ ¼å¼ï¼ˆjson/csvï¼‰',
            'includednsinfo': 'ä¿å­˜åŸŸåæ–‡ä»¶æ—¶æ˜¯å¦åŒ…å«DNSæŸ¥è¯¢ç»“æœï¼ˆtrue/falseï¼‰'
        }
            
        while True:
            section_name = self.config.get_name(section)
            print(f"\nğŸ“ ç¼–è¾‘ {section_name} é…ç½®")
            print("="*50)
            
            options = self.config.config.options(section)
            for i, option in enumerate(options, 1):
                value = self.config.get(section, option)
                # ç›´æ¥ä½¿ç”¨ç¿»è¯‘å­—å…¸
                option_chinese_name = key_translations.get(option.lower(), option)
                option_desc = desc_translations.get(option.lower(), "")
                print(f"{i}. {option_chinese_name} = {value}")
                if option_desc:
                    print(f"   - {option_desc}")
            print(f"{len(options)+1}. â¬…ï¸ è¿”å›ä¸Šçº§èœå•")
            
            try:
                choice = int(input("\nè¯·é€‰æ‹©è¦ç¼–è¾‘çš„é€‰é¡¹: "))
                
                if 1 <= choice <= len(options):
                    option = options[choice-1]
                    # ç›´æ¥ä½¿ç”¨ç¿»è¯‘å­—å…¸
                    option_chinese_name = key_translations.get(option.lower(), option)
                    current_value = self.config.get(section, option)
                    option_desc = desc_translations.get(option.lower(), "")
                    if option_desc:
                        print(f"æè¿°: {option_desc}")
                    new_value = input(f"è¯·è¾“å…¥æ–°çš„{option_chinese_name}çš„å€¼ (å½“å‰å€¼: {current_value}): ")
                    self.config.set(section, option, new_value)
                    print(f"âœ… å·²æ›´æ–° {option_chinese_name} = {new_value}")
                elif choice == len(options)+1:
                    break
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©!")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")

    def run_performance_test(self):
        """è¿è¡Œæ€§èƒ½æµ‹è¯•ï¼Œæ‰¾å‡ºæœ€ä½³å‚æ•°é…ç½®"""
        print("\n" + "="*50)
        print("ğŸš€ DNSç¼“å­˜å·¥å…·å‚æ•°æ€§èƒ½æµ‹è¯•")
        print("="*50)
        
        # è¯¢é—®æ˜¯å¦ä½¿ç”¨å½“å‰åŸŸåæ•°æ®
        use_current_domains = False
        test_file = None
        
        if self.collected_domains:
            while True:
                choice = input(f"æ˜¯å¦ä½¿ç”¨å½“å‰å·²æ”¶é›†çš„ {len(self.collected_domains)} ä¸ªåŸŸåè¿›è¡Œæµ‹è¯•ï¼Ÿ(y/n): ")
                if choice.lower() in ['y', 'yes', 'æ˜¯', 'æ˜¯çš„']:
                    use_current_domains = True
                    
                    # ä¿å­˜å½“å‰åŸŸååˆ°ä¸´æ—¶æ–‡ä»¶
                    temp_file = os.path.join(self.data_dir, "temp_test_domains.json")
                    with open(temp_file, 'w', encoding='utf-8') as f:
                        json.dump(list(self.collected_domains), f)
                    test_file = temp_file
                    print(f"å·²å°†å½“å‰åŸŸåä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_file}")
                    break
                elif choice.lower() in ['n', 'no', 'å¦', 'ä¸']:
                    break
                else:
                    print("æ— æ•ˆçš„è¾“å…¥ï¼Œè¯·è¾“å…¥ y æˆ– n")
        
        # å¦‚æœä¸ä½¿ç”¨å½“å‰åŸŸåï¼Œè¯¢é—®æ˜¯å¦ä½¿ç”¨å·²æœ‰æ–‡ä»¶
        if not use_current_domains:
            files = self.get_available_files()
            if files:
                print("\nğŸ“‚ å¯ä»¥ä½¿ç”¨çš„åŸŸåæ–‡ä»¶:")
                for i, file in enumerate(files, 1):
                    print(f"{i}. {os.path.basename(file)}")
                print(f"{len(files)+1}. ä½¿ç”¨é»˜è®¤æµ‹è¯•åŸŸå")
                
                while True:
                    try:
                        choice = int(input("\nè¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ–‡ä»¶ (è¾“å…¥åºå·): "))
                        if 1 <= choice <= len(files):
                            test_file = files[choice-1]
                            print(f"é€‰æ‹©äº†æ–‡ä»¶: {test_file}")
                            break
                        elif choice == len(files)+1:
                            print("å°†ä½¿ç”¨é»˜è®¤æµ‹è¯•åŸŸå")
                            break
                        else:
                            print("æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡è¯•")
                    except ValueError:
                        print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")
        
        # åˆ›å»ºå¹¶è¿è¡Œæ€§èƒ½æµ‹è¯•å™¨
        tester = DNSPerformanceTester(test_file, "test_results", self.config)
        tester.run_tests()
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if use_current_domains and os.path.exists(test_file):
            try:
                os.remove(test_file)
                print(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {test_file}")
            except:
                pass
        
        input("\næŒ‰Enteré”®è¿”å›ä¸»èœå•...")

def main():
    tool = DNSCacheTool()
    
    while True:
        print("\n" + "="*50)
        print("ğŸŒ DNSç¼“å­˜å·¥å…· ğŸš€")
        print("="*50)
        print("1. ğŸ” ä»æ–°åŸŸåå¼€å§‹æ”¶é›†")
        print("2. ğŸ”„ ä½¿ç”¨å·²æœ‰åŸŸåæ–‡ä»¶æŸ¥è¯¢DNS")
        print("3. ğŸ“¥ å¯¼å…¥åŸŸååˆ—è¡¨")
        print("4. ğŸ“¤ å¯¼å‡ºä¸Šæ¬¡æŸ¥è¯¢ç»“æœ")
        print("5. âš™ï¸ é…ç½®è®¾ç½®")
        print("6. ğŸš€ è¿è¡Œæ€§èƒ½æµ‹è¯•")
        print("7. ğŸ‘‹ é€€å‡º")
        
        choice = input("\nè¯·é€‰æ‹©æ“ä½œ: ")
        
        if choice == '1':
            start_domain = input("è¯·è¾“å…¥èµ·å§‹åŸŸå (ä¾‹å¦‚: example.com): ")
            if not start_domain:
                print("âŒ åŸŸåä¸èƒ½ä¸ºç©º!")
                continue
            
            # è¯¢é—®æ˜¯å¦åªæ”¶é›†å­åŸŸå
            only_subdomains_choice = input(f"æ˜¯å¦åªæ”¶é›† {start_domain} çš„å­åŸŸå? (y/n): ").lower()
            only_subdomains = only_subdomains_choice == 'y'
            
            tool.current_file = None  # é‡ç½®å½“å‰æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶
            tool.collect_domains(start_domain, only_subdomains)
        
        elif choice == '2':
            files = tool.get_available_files()
            
            if not files:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°åŸŸåæ–‡ä»¶! è¯·å…ˆæ”¶é›†åŸŸåã€‚")
                continue
            
            print("\nğŸ“‚ å¯ç”¨çš„æ–‡ä»¶:")
            for i, file in enumerate(files, 1):
                print(f"{i}. {os.path.basename(file)}")
            
            try:
                file_index = int(input("\nè¯·é€‰æ‹©æ–‡ä»¶ (è¾“å…¥åºå·): ")) - 1
                if 0 <= file_index < len(files):
                    tool.batch_query_dns(files[file_index])
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©!")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„åºå·!")
        
        elif choice == '3':
            tool.import_domains()
        
        elif choice == '4':
            if not tool.dns_results:
                print("âŒ æ²¡æœ‰å¯å¯¼å‡ºçš„DNSæŸ¥è¯¢ç»“æœ! è¯·å…ˆè¿›è¡ŒæŸ¥è¯¢ã€‚")
                continue
            
            tool.ask_export_results()
        
        elif choice == '5':
            tool.edit_config()
        
        elif choice == '6':
            tool.run_performance_test()
        
        elif choice == '7':
            print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨! å†è§!")
            break
        
        else:
            print("âŒ æ— æ•ˆçš„é€‰æ‹©! è¯·é‡è¯•ã€‚")

if __name__ == "__main__":
    main() 
